{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc080426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.split import eval_splits, get_all_combs, get_valid_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fccf176",
   "metadata": {},
   "source": [
    "1. Find combination of comments that reach 85% and 15% of the train data $\\approx$ 60% and 10% of the full dataset.\n",
    "2. Find which combination has the most similar topic distribution to the original train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f2b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = pd.read_csv(\"data/detests/train_with_disagreement_context_soft.csv\").fillna(\"\")\n",
    "# train_val_no_fill = pd.read_csv(\"../data/detests/train_with_disagreement_context_no_fill_soft.csv\").fillna(\"\")\n",
    "labels = [\n",
    "    \"xenophobia\",\n",
    "    \"suffering\",\n",
    "    \"economic\",\n",
    "    \"migration\",\n",
    "    \"culture\",\n",
    "    \"benefits\",\n",
    "    \"health\",\n",
    "    \"security\",\n",
    "    \"dehumanisation\",\n",
    "    \"others\",\n",
    "]\n",
    "labels_groups = labels + [\"implicit\"]\n",
    "y_columns = [\"stereo\"] + labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e384f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The category distribution of the data in general is: \\n\")\n",
    "gen_dist = train_val[labels_groups].apply(pd.Series.value_counts)\n",
    "gen_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5b5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_dist *= 100 / len(train_val)\n",
    "gen_dist.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462706e6",
   "metadata": {},
   "source": [
    "## Split with comment_id and threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d67b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of distinct comments is {train_val[\"comment_id\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1804fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = train_val[[\"comment_id\", \"reply_to\"]].groupby(\"comment_id\").tail(1)\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(comments.to_numpy().tolist())\n",
    "comps = list(nx.connected_components(G))\n",
    "print(f\"The number of distinct threads is {len(comps)}\")\n",
    "comps[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf62898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = np.array([len(c) for c in comps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c3fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "(le == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2282f15e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def number_comp(comment_id):\n",
    "    i = 0\n",
    "    while comment_id not in comps[i]:\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "\n",
    "train_val[\"thread\"] = train_val[\"comment_id\"].apply(number_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a33df",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "1. Find combination of news that reach 15% of the data\n",
    "2. Find which of these combinations has the most similar topic distribution\n",
    "\n",
    "+ Problem: too many combinations\n",
    "+ Approach: Batch of threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e185d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_files_sz(df, column, batch_size=10):\n",
    "    file_sz = df.groupby(column).size().sample(frac=1, random_state=42)\n",
    "    n_batches = (len(file_sz) - 1) // batch_size + 1\n",
    "    return np.array_split(file_sz, n_batches)\n",
    "\n",
    "\n",
    "batches = batch_files_sz(train_val, \"thread\", 35)\n",
    "file_sz = pd.Series([batch.sum() for batch in batches])\n",
    "keys = [batch.keys().tolist() for batch in batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418fdd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_splits = get_all_combs(file_sz, test_ratio=0.15, eps=0.01)\n",
    "valid_splits2 = [set(itertools.chain(*[keys[batch] for batch in split[0]])) for split in valid_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0282698",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = eval_splits(train_val, valid_splits2, \"thread\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea06d9",
   "metadata": {},
   "source": [
    "# Split with news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a58ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of distinct news is {train_val[\"file_id\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b28219",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sz = train_val.groupby(\"file_id\").size().sort_values()\n",
    "file_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_splits = get_all_combs(file_sz, test_ratio=0.15, eps=0.1)\n",
    "valid_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_splits2 = [split[0] for split in valid_splits]\n",
    "res = eval_splits(train_val, valid_splits2, \"file_id\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16a1e36",
   "metadata": {},
   "source": [
    "# Check results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea948acf",
   "metadata": {},
   "source": [
    "Sort results by MSE and MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb2689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.sort_values(by=\"MSE\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2cd5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.sort_values(by=\"MAPE\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e723822",
   "metadata": {},
   "source": [
    "We keep the one with lower MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ab844",
   "metadata": {},
   "outputs": [],
   "source": [
    "field = \"file_id\"\n",
    "# field = \"thread\"\n",
    "val_split = valid_splits2[0]\n",
    "print(val_split)\n",
    "val = train_val[train_val[field].isin(val_split)]\n",
    "train = train_val[~train_val[field].isin(val_split)]\n",
    "\n",
    "# val_no_fill = train_val_no_fill[train_val[field].isin(val_split)]\n",
    "# train_no_fill = train_val_no_fill[~train_val[field].isin(val_split)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e5b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"data/detests/train_split_context_soft.csv\", index=False)\n",
    "val.to_csv(\"data/detests/val_split_context_soft.csv\", index=False)\n",
    "\n",
    "# train_no_fill.to_csv(\"data/detests/train_split_context_no_fill_soft.csv\", index=False)\n",
    "# val_no_fill.to_csv(\"data/detests/val_split_context_no_fill_soft.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6643a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of train_val (original TRAIN dataset)\n",
    "perc_train_val = np.array([len(train), len(val)])\n",
    "perc_train_val = perc_train_val / len(train_val) * 100\n",
    "perc_train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e34cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of whole dataset\n",
    "perc_train_val * 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e49527",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[labels_groups].apply(pd.Series.value_counts) / len(train) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa9aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val[labels_groups].apply(pd.Series.value_counts) / len(val) * 100"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
